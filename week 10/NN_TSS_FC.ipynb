{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before optimization, w is [1.0, 1.0]\n",
      "Before optimization, b is 0.0\n",
      "Epoch [10/500], Loss: 2.0397\n",
      "Epoch [20/500], Loss: 1.6624\n",
      "Epoch [30/500], Loss: 1.3642\n",
      "Epoch [40/500], Loss: 1.1407\n",
      "Epoch [50/500], Loss: 0.9818\n",
      "Epoch [60/500], Loss: 0.8747\n",
      "Epoch [70/500], Loss: 0.8064\n",
      "Epoch [80/500], Loss: 0.7652\n",
      "Epoch [90/500], Loss: 0.7415\n",
      "Epoch [100/500], Loss: 0.7286\n",
      "Epoch [110/500], Loss: 0.7217\n",
      "Epoch [120/500], Loss: 0.7180\n",
      "Epoch [130/500], Loss: 0.7157\n",
      "Epoch [140/500], Loss: 0.7142\n",
      "Epoch [150/500], Loss: 0.7130\n",
      "Epoch [160/500], Loss: 0.7120\n",
      "Epoch [170/500], Loss: 0.7111\n",
      "Epoch [180/500], Loss: 0.7103\n",
      "Epoch [190/500], Loss: 0.7095\n",
      "Epoch [200/500], Loss: 0.7089\n",
      "Epoch [210/500], Loss: 0.7083\n",
      "Epoch [220/500], Loss: 0.7078\n",
      "Epoch [230/500], Loss: 0.7074\n",
      "Epoch [240/500], Loss: 0.7070\n",
      "Epoch [250/500], Loss: 0.7067\n",
      "Epoch [260/500], Loss: 0.7064\n",
      "Epoch [270/500], Loss: 0.7062\n",
      "Epoch [280/500], Loss: 0.7060\n",
      "Epoch [290/500], Loss: 0.7058\n",
      "Epoch [300/500], Loss: 0.7057\n",
      "Epoch [310/500], Loss: 0.7056\n",
      "Epoch [320/500], Loss: 0.7055\n",
      "Epoch [330/500], Loss: 0.7054\n",
      "Epoch [340/500], Loss: 0.7053\n",
      "Epoch [350/500], Loss: 0.7053\n",
      "Epoch [360/500], Loss: 0.7052\n",
      "Epoch [370/500], Loss: 0.7052\n",
      "Epoch [380/500], Loss: 0.7052\n",
      "Epoch [390/500], Loss: 0.7051\n",
      "Epoch [400/500], Loss: 0.7051\n",
      "Epoch [410/500], Loss: 0.7051\n",
      "Epoch [420/500], Loss: 0.7051\n",
      "Epoch [430/500], Loss: 0.7051\n",
      "Epoch [440/500], Loss: 0.7051\n",
      "Epoch [450/500], Loss: 0.7051\n",
      "Epoch [460/500], Loss: 0.7051\n",
      "Epoch [470/500], Loss: 0.7051\n",
      "Epoch [480/500], Loss: 0.7051\n",
      "Epoch [490/500], Loss: 0.7050\n",
      "Epoch [500/500], Loss: 0.7050\n",
      "After optimization, w is [0.11765977740287781, 0.4547910988330841]\n",
      "After optimization, b is 5.157975380143398e-08\n",
      "First few predictions: [-0.338822    0.15502293  0.07787751  0.08738358 -0.06800136]\n",
      "First few actual values: [-0.24834451  1.4593257   1.1959049   1.3459936   1.3158773 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the data from the Excel file\n",
    "file_path = \"C:/Users/rileybla/Desktop/CIE500_SP2025/week 10/BSA Analysis Results 2023-2025.xlsx\"\n",
    "iris = pd.read_excel(file_path, sheet_name=\"BSA Master Log\")\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = [\"Turbidity (NTU)\", \"TSS (mg solids/L)\", \"Calc. FC (CFU/100 mL)\"]\n",
    "iris = iris.dropna(subset=columns_of_interest)\n",
    "\n",
    "\n",
    "# Log-transform target variable\n",
    "iris[\"Calc. FC (CFU/100 mL)\"] = np.log1p(iris[\"Calc. FC (CFU/100 mL)\"])\n",
    "\n",
    "# Normalize features and target using StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X = scaler_X.fit_transform(iris[[\"Turbidity (NTU)\", \"TSS (mg solids/L)\"]])  # Features\n",
    "y = scaler_y.fit_transform(iris[[\"Calc. FC (CFU/100 mL)\"]])  # Target\n",
    "\n",
    "# Convert to PyTorch tensors with consistent dtype (float32)\n",
    "x = torch.tensor(X, dtype=torch.float32, requires_grad=False)  # Features\n",
    "y = torch.tensor(y.flatten(), dtype=torch.float32, requires_grad=False)  # Target\n",
    "\n",
    "# Initialize weights and bias with consistent dtype\n",
    "w = torch.tensor([1.0, 1.0], dtype=torch.float32, requires_grad=True)  # Weights for two features\n",
    "b = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)  # Bias\n",
    "\n",
    "print(f'Before optimization, w is {w.tolist()}')\n",
    "print(f'Before optimization, b is {b.item()}')\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam([w, b], lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "epochs = 500\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    \n",
    "    # Forward pass: calculate predictions using matrix multiplication\n",
    "    y_pred = torch.matmul(x, w) + b\n",
    "    \n",
    "    # Compute loss (Mean Squared Error)\n",
    "    loss = ((y_pred - y) ** 2).mean()\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights and bias using optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Print final values of weights and bias\n",
    "print(f'After optimization, w is {w.tolist()}')\n",
    "print(f'After optimization, b is {b.item()}')\n",
    "\n",
    "# Predictions using optimized parameters\n",
    "with torch.no_grad():\n",
    "    y_pred_final = torch.matmul(x, w) + b\n",
    "\n",
    "print(\"First few predictions:\", y_pred_final[:5].numpy())\n",
    "print(\"First few actual values:\", y[:5].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# are there more related variables to improve prediction\n",
    "# Load the data from the Excel file\n",
    "file_path = \"C:/Users/rileybla/Desktop/CIE500_SP2025/week 10/BSA Analysis Results 2023-2025.xlsx\"\n",
    "iris = pd.read_excel(file_path, sheet_name=\"BSA Master Log\")\n",
    "\n",
    "# Select columns of interest\n",
    "columns_of_interest = [\"Turbidity (NTU)\", \"TSS (mg solids/L)\", \"Calc. FC (CFU/100 mL)\"]\n",
    "iris = iris.dropna(subset=columns_of_interest)\n",
    "\n",
    "# Log-transform target variable\n",
    "iris[\"Calc. FC (CFU/100 mL)\"] = np.log1p(iris[\"Calc. FC (CFU/100 mL)\"]) # test if need to do this beause fit_transform may take care of this\n",
    "\n",
    "# Normalize features and target using StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X = scaler_X.fit_transform(iris[[\"Turbidity (NTU)\", \"TSS (mg solids/L)\"]])  # Features    reverse function of this check online\n",
    "y = scaler_y.fit_transform(iris[[\"Calc. FC (CFU/100 mL)\"]])  # Target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This format scaler_y.inverse_transform(output) and include log undo\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.flatten(), dtype=torch.float32)\n",
    "x_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.flatten(), dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase number of neurons and layers for more accuracy\n",
    "# Define a complex neural network model\n",
    "class ComplexNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ComplexNN, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(2, 30)  # First hidden layer with 20 neurons \n",
    "        self.hidden2 = torch.nn.Linear(30, 10)  # Second hidden layer with 10 neurons\n",
    "        self.output = torch.nn.Linear(10, 1)   # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden1(x))       # ReLU activation for first hidden layer\n",
    "        x = torch.relu(self.hidden2(x))       # ReLU activation for second hidden layer\n",
    "        x = self.output(x)                    # Output layer\n",
    "        return x\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = ComplexNN()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()  # Mean Squared Error Loss RMSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/600], Training Loss: 0.2179\n",
      "Epoch [20/600], Training Loss: 0.2064\n",
      "Epoch [30/600], Training Loss: 0.2063\n",
      "Epoch [40/600], Training Loss: 0.2054\n",
      "Epoch [50/600], Training Loss: 0.2040\n",
      "Epoch [60/600], Training Loss: 0.2032\n",
      "Epoch [70/600], Training Loss: 0.2031\n",
      "Epoch [80/600], Training Loss: 0.2031\n",
      "Epoch [90/600], Training Loss: 0.2011\n",
      "Epoch [100/600], Training Loss: 0.2008\n",
      "Epoch [110/600], Training Loss: 0.1990\n",
      "Epoch [120/600], Training Loss: 0.1986\n",
      "Epoch [130/600], Training Loss: 0.1980\n",
      "Epoch [140/600], Training Loss: 0.2016\n",
      "Epoch [150/600], Training Loss: 0.1979\n",
      "Epoch [160/600], Training Loss: 0.1991\n",
      "Epoch [170/600], Training Loss: 0.1987\n",
      "Epoch [180/600], Training Loss: 0.1978\n",
      "Epoch [190/600], Training Loss: 0.1973\n",
      "Epoch [200/600], Training Loss: 0.1948\n",
      "Epoch [210/600], Training Loss: 0.1961\n",
      "Epoch [220/600], Training Loss: 0.1948\n",
      "Epoch [230/600], Training Loss: 0.1966\n",
      "Epoch [240/600], Training Loss: 0.1933\n",
      "Epoch [250/600], Training Loss: 0.1930\n",
      "Epoch [260/600], Training Loss: 0.1924\n",
      "Epoch [270/600], Training Loss: 0.1909\n",
      "Epoch [280/600], Training Loss: 0.1926\n",
      "Epoch [290/600], Training Loss: 0.1910\n",
      "Epoch [300/600], Training Loss: 0.1928\n",
      "Epoch [310/600], Training Loss: 0.1918\n",
      "Epoch [320/600], Training Loss: 0.1904\n",
      "Epoch [330/600], Training Loss: 0.1890\n",
      "Epoch [340/600], Training Loss: 0.1882\n",
      "Epoch [350/600], Training Loss: 0.1879\n",
      "Epoch [360/600], Training Loss: 0.1877\n",
      "Epoch [370/600], Training Loss: 0.1889\n",
      "Epoch [380/600], Training Loss: 0.1938\n",
      "Epoch [390/600], Training Loss: 0.1939\n",
      "Epoch [400/600], Training Loss: 0.1896\n",
      "Epoch [410/600], Training Loss: 0.1861\n",
      "Epoch [420/600], Training Loss: 0.1860\n",
      "Epoch [430/600], Training Loss: 0.1885\n",
      "Epoch [440/600], Training Loss: 0.1853\n",
      "Epoch [450/600], Training Loss: 0.1900\n",
      "Epoch [460/600], Training Loss: 0.1859\n",
      "Epoch [470/600], Training Loss: 0.1848\n",
      "Epoch [480/600], Training Loss: 0.1845\n",
      "Epoch [490/600], Training Loss: 0.1839\n",
      "Epoch [500/600], Training Loss: 0.1851\n",
      "Epoch [510/600], Training Loss: 0.1849\n",
      "Epoch [520/600], Training Loss: 0.1836\n",
      "Epoch [530/600], Training Loss: 0.1932\n",
      "Epoch [540/600], Training Loss: 0.1976\n",
      "Epoch [550/600], Training Loss: 0.1817\n",
      "Epoch [560/600], Training Loss: 0.1842\n",
      "Epoch [570/600], Training Loss: 0.1811\n",
      "Epoch [580/600], Training Loss: 0.1830\n",
      "Epoch [590/600], Training Loss: 0.1803\n",
      "Epoch [600/600], Training Loss: 0.1802\n"
     ]
    }
   ],
   "source": [
    "# also split into multiple batches. train iteratively use batch size: number of training example in one forward/backward pass\n",
    "# Training loop\n",
    "model.train()  # Set model to training mode all the model can be trained\n",
    "\n",
    "epochs = 600\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "    \n",
    "    # Forward pass: calculate predictions on training set\n",
    "    y_pred_train = model(x_train)\n",
    "    \n",
    "    # Compute loss on training set\n",
    "    loss_train = criterion(y_pred_train.flatten(), y_train)\n",
    "    \n",
    "    # Backward pass: compute gradients\n",
    "    loss_train.backward()\n",
    "    \n",
    "    # Update weights using optimizer\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Training Loss: {loss_train.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.9675\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Evaluate model on test set\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model(x_test).flatten()  # Predictions on test set\n",
    "    \n",
    "    # Compute loss on test set\n",
    "    loss_test = criterion(y_pred_test, y_test)\n",
    "    print(f\"Test Loss: {loss_test.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few predictions: [ 4.2992525  -0.38058853  0.20363145  0.94705623 -0.75729764]\n",
      "First few actual values: [ 1.2027607  -0.30847684  1.3158773   1.2296234  -1.3362627 ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Print first few predictions and actual values from test set (scaled values)\n",
    "print(\"First few predictions:\", y_pred_test[:5].numpy())\n",
    "print(\"First few actual values:\", y_test[:5].numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few predictions (original scale): [1.1171509e+13 2.1245959e+03 3.4751078e+04 1.2166596e+06 3.4975797e+02]\n",
      "First few actual values (original scale): [4.1333348e+06 2.9999990e+03 7.0999980e+06 4.7000005e+06 2.0999996e+01]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reverse scaling for predictions and actual values to interpret results in original scale\n",
    "y_pred_scaled = scaler_y.inverse_transform(y_pred_test.numpy().reshape(-1, 1))\n",
    "y_actual_scaled = scaler_y.inverse_transform(y_test.numpy().reshape(-1, 1))\n",
    "\n",
    "# Reverse log1p transform using exponential\n",
    "y_pred_original = np.expm1(y_pred_scaled)  # Undo log1p: e^(x) - 1\n",
    "y_actual_original = np.expm1(y_actual_scaled)\n",
    "\n",
    "print(\"First few predictions (original scale):\", y_pred_original[:5].flatten())\n",
    "print(\"First few actual values (original scale):\", y_actual_original[:5].flatten())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RBBenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
